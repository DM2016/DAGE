#!/bin/bash
DIR=/Users/dichenli/Documents/TPOT_project/DAGE/ScalaSparkDAGE

cd $DIR
sbt assembly
# delete folder, this is necessary because spark will fail if output directory already exists
aws s3 rm s3://dage-spark-data/output/vep1 --recursive
aws s3 mb s3://dage-spark-data/output/
aws s3 cp ./target/scala-2.10/Simple_Project-assembly-0.0.1.jar s3://dage-spark-data/jars/Simple_Project-assembly-0.0.1.jar
# /users/dichenli/.ec2/scp_trycassandra_0 ./target/scala-2.10/Simple_Project-assembly-0.0.1.jar

# aws emr create-cluster --applications Name=Hadoop Name=Hive Name=Pig Name=Hue Name=Spark --ec2-attributes '{"KeyName":"trycassandra","InstanceProfile":"EMR_EC2_DefaultRole","SubnetId":"subnet-884232ff","EmrManagedSlaveSecurityGroup":"sg-3c662558","EmrManagedMasterSecurityGroup":"sg-33662557"}' --service-role EMR_DefaultRole --enable-debugging --release-label emr-4.3.0 --log-uri 's3n://dage-spark-data/logs/' --steps '[{"Args":["spark-submit","--deploy-mode","cluster","s3://dage-spark-data/jars/Simple_Project-assembly-0.0.1.jar"],"Type":"CUSTOM_JAR","ActionOnFailure":"CONTINUE","Jar":"command-runner.jar","Properties":"","Name":"Spark application 1"}]' --name 'My cluster' --instance-groups '[{"InstanceCount":1,"InstanceGroupType":"MASTER","InstanceType":"m3.large","Name":"Master instance group - 1"},{"InstanceCount":1,"InstanceGroupType":"CORE","InstanceType":"m3.large","Name":"Core instance group - 2"}]' --region us-east-1